{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "937d6f897d6246258843cc2432debaca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8fadfb9a9b934886a3455dff3d75bc18",
              "IPY_MODEL_470826472d434ec1a319ef2e609f32bb",
              "IPY_MODEL_139d9dc7d4454ad395b1b52e49f65b26"
            ],
            "layout": "IPY_MODEL_c0c60b857e2e43369fccbfd789dd8480"
          }
        },
        "8fadfb9a9b934886a3455dff3d75bc18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_537ce7e6a65a497cae61f33a902897c8",
            "placeholder": "​",
            "style": "IPY_MODEL_f3db47d8aedf42c0a48c21641f4d5409",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "470826472d434ec1a319ef2e609f32bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30bdba5048fa4ce883a38b1726781ef9",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6da71bdc3e63481194afd11975f43194",
            "value": 2
          }
        },
        "139d9dc7d4454ad395b1b52e49f65b26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bcaf0839d954fa69b5e7eb42da6748d",
            "placeholder": "​",
            "style": "IPY_MODEL_c363ce43192343d8b030b816f54cef9f",
            "value": " 2/2 [01:07&lt;00:00, 31.47s/it]"
          }
        },
        "c0c60b857e2e43369fccbfd789dd8480": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "537ce7e6a65a497cae61f33a902897c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3db47d8aedf42c0a48c21641f4d5409": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30bdba5048fa4ce883a38b1726781ef9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6da71bdc3e63481194afd11975f43194": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1bcaf0839d954fa69b5e7eb42da6748d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c363ce43192343d8b030b816f54cef9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MistralForCausalLM Inference Optimization\n",
        "# -----------------------------------------------------\n",
        "# This script optimizes MistralForCausalLM inference to achieve 200+ tokens/sec\n",
        "# on a T4 GPU with 16GB VRAM, handling 32 concurrent requests of 128 tokens each."
      ],
      "metadata": {
        "id": "HPUOeLjl4qme"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies Installation\n",
        "# Install required libraries for transformer model loading, quantization, and optimization"
      ],
      "metadata": {
        "id": "V9OPghU74fN5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "O3NvRPbOX6mE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d752bec6-e004-4898-a041-54443c0f85f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers accelerate safetensors einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate safetensors optimum einops flash-attn bitsandbytes"
      ],
      "metadata": {
        "id": "1vEdj3iudQXi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b1db72b-6ccd-4dc9-955a-037a7b9a7291"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m4.9/6.0 MB\u001b[0m \u001b[31m141.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m137.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.6/433.6 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n"
      ],
      "metadata": {
        "id": "raQyQAYs-XWO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "IyTcmYVOrrwT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q hqq transformers[torch] optimum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOSGztzpkhnt",
        "outputId": "f85ed0b5-9d56-4981-aa86-6d5ac04785d9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for hqq (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Setup and GPU Verification\n",
        "# Verify GPU availability and display memory information"
      ],
      "metadata": {
        "id": "0Sb-_eqG408J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "MistralForCausalLM Optimization Script\n",
        "\n",
        "Optimizes inference for Mistral-7B models on T4 GPUs, achieving 200+ tokens/sec throughput\n",
        "with 32 concurrent requests. Implements memory-efficient KV cache handling, quantization,\n",
        "and T4-specific optimizations.\n",
        "\n",
        "Features:\n",
        "- 4-bit quantization with NF4 precision\n",
        "- Static KV cache implementation\n",
        "- Memory alignment optimization\n",
        "- torch.compile with kernel fusion\n",
        "- LoRA adapter support\n",
        "\n",
        "Author: Ankit\n",
        "Date: 27/03/2025\n",
        "\"\"\"\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import threading\n",
        "from queue import Queue\n",
        "from threading import Thread\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoConfig,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Check GPU info\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "    print(f\"GPU: {gpu_name}\")\n",
        "    print(f\"Memory: {gpu_memory:.2f} GB\")\n",
        "else:\n",
        "    print(\"No GPU detected, using CPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yppa2UYSX_Jb",
        "outputId": "6af4dfb5-d821-417f-e2c2-c7078b3eef3c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "Memory: 14.74 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration\n",
        "# Define configuration parameters for model loading and inference optimization\n",
        "# This dataclass centralizes all tunable parameters for easy experimentation"
      ],
      "metadata": {
        "id": "A0jfTDhB8aYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class InferenceConfig:\n",
        "    model_id: str\n",
        "    dtype: torch.dtype = torch.float16\n",
        "    max_input_length: int = 128  # Exactly 128 tokens for memory alignment\n",
        "    max_output_length: int = 128  # Matches the assignment requirements\n",
        "    batch_size: int = 32  # Optimized batch size for T4 throughput\n",
        "    use_lora: bool = False\n",
        "    lora_path: Optional[str] = None\n",
        "    # KV cache optimizations\n",
        "    cache_implementation: str = \"static\"  # Static cache for memory coalescing\n",
        "    use_flash_attention: bool = False  # Disabled as incompatible with T4\n",
        "    # Additional optimization flags\n",
        "    use_sdpa: bool = False  # Scaled Dot Product Attention (disabled for T4)\n",
        "    memory_efficient: bool = True\n",
        "\n",
        "## Model Loading and Optimization\n",
        "# Load and optimize the model with 4-bit quantization and compilation\n",
        "# T4-specific optimizations applied to maximize throughput\n",
        "\n",
        "def load_and_optimize_model(config: InferenceConfig):\n",
        "    \"\"\"\n",
        "    Load and optimize the model for inference with advanced T4-specific optimizations.\n",
        "    \"\"\"\n",
        "    print(f\"Loading model: {config.model_id}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Set up better memory management\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Try to enable TF32 precision if available\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True   # Enables TF32 math operations\n",
        "        torch.backends.cudnn.allow_tf32 = True   # Enables TF32 for cuDNN ops\n",
        "\n",
        "    # Set quantization config to optimize memory usage - critical for T4's 16GB limit\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,    # 4-bit quantization reduces memory by ~4x\n",
        "        bnb_4bit_quant_type=\"nf4\",    # Normalized float format for better quality\n",
        "        bnb_4bit_compute_dtype=config.dtype,   # Use float16 precision\n",
        "        bnb_4bit_use_double_quant=True,    # Enable double quantization for further savings\n",
        "    )\n",
        "\n",
        "    # Load model with optimizations\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        config.model_id,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=quantization_config,\n",
        "        torch_dtype=config.dtype,\n",
        "        trust_remote_code=True,\n",
        "        low_cpu_mem_usage=True\n",
        "    )\n",
        "\n",
        "    # Apply torch.compile with max-autotune for kernel fusion and optimization\n",
        "    # This enables better utilization of T4's compute capabilities\n",
        "    try:\n",
        "        print(\"Applying torch.compile with max-autotune...\")\n",
        "        model = torch.compile(model, fullgraph=True, mode=\"max-autotune\")\n",
        "        print(\"Compilation successful\")\n",
        "    except Exception as e:\n",
        "        print(f\"Torch compile failed (continuing without it): {e}\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        config.model_id,\n",
        "        trust_remote_code=True,\n",
        "        use_fast=True\n",
        "    )\n",
        "\n",
        "    if not tokenizer.pad_token:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Note: We'll use pad_to_multiple_of parameter during tokenization instead\n",
        "    print(\"Using 128-token padding alignment for optimal memory access\")\n",
        "\n",
        "    # Move model to GPU if available\n",
        "    model.eval()\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Model loaded and optimized in {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "2-VLlsRsYAci"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Warmup\n",
        "# Perform strategic model warmup to optimize inference performance\n",
        "# Includes progressive batch size increase to prepare caches and JIT compilation"
      ],
      "metadata": {
        "id": "e-M8fcqC9P_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def warmup_model(model, tokenizer, config: InferenceConfig):\n",
        "    \"\"\"\n",
        "    Strategic warmup to stabilize performance and prepare caches for inference.\n",
        "    This significantly improves initial throughput and reduces variance.\n",
        "    \"\"\"\n",
        "    print(\"Starting model warmup...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Create a sample input for warmup\n",
        "    sample_text = \"This is a sample input to warm up the model.\" * 8  # Make it long enough for realistic caching\n",
        "    sample_inputs = tokenizer(\n",
        "        [sample_text] * config.batch_size,  # Batch of identical inputs\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",\n",
        "        max_length=config.max_input_length,\n",
        "        truncation=True\n",
        "    ).to(device)\n",
        "\n",
        "    # Perform several warmup runs with different batch sizes\n",
        "    warmup_batch_sizes = [1, min(2, config.batch_size), min(4, config.batch_size)]\n",
        "    for batch_size in warmup_batch_sizes:\n",
        "        print(f\"Warmup with batch size {batch_size}\")\n",
        "        with torch.no_grad():\n",
        "            for _ in range(3):  # Multiple runs for each batch size\n",
        "                # Slice the inputs to match current batch size\n",
        "                batch_inputs = {k: v[:batch_size] for k, v in sample_inputs.items()}\n",
        "\n",
        "                # Generate output with default settings - avoid anything that might cause errors\n",
        "                output = model.generate(\n",
        "                    **batch_inputs,\n",
        "                    max_new_tokens=config.max_output_length,\n",
        "                    do_sample=False\n",
        "                )\n",
        "\n",
        "    # Run a benchmark on a large batch to stabilize performance\n",
        "    if config.batch_size >= 4:\n",
        "        print(\"Running final warmup with maximum batch size\")\n",
        "        batch_inputs = {k: v[:config.batch_size] for k, v in sample_inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(\n",
        "                **batch_inputs,\n",
        "                max_new_tokens=config.max_output_length,\n",
        "                do_sample=False\n",
        "            )\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Warmup completed in {end_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "xbRLCrusYCB0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Request Processing\n",
        "# Define request object and implement optimized batch processing\n",
        "# Memory access patterns and static KV cache optimizations applied"
      ],
      "metadata": {
        "id": "ALK5ufNb9m8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class InferenceRequest:\n",
        "    \"\"\"Tracks a single inference request through the system with timing and metrics\"\"\"\n",
        "    id: int\n",
        "    prompt: str\n",
        "    result: str = \"\"\n",
        "    metrics: Dict[str, Any] = None\n",
        "    start_time: float = None\n",
        "    end_time: float = None\n",
        "\n",
        "def process_requests(model, tokenizer, request_queue: Queue, results: List[InferenceRequest], config: InferenceConfig):\n",
        "    \"\"\"\n",
        "    Process batches of requests with optimized memory access patterns.\n",
        "    \"\"\"\n",
        "    # Create a thread-local tokenizer\n",
        "    local_tokenizer = AutoTokenizer.from_pretrained(\n",
        "        config.model_id,\n",
        "        trust_remote_code=True,\n",
        "        use_fast=True\n",
        "    )\n",
        "    if not local_tokenizer.pad_token:\n",
        "        local_tokenizer.pad_token = local_tokenizer.eos_token\n",
        "\n",
        "    print(f\"Initial GPU memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB allocated\")\n",
        "\n",
        "    while not request_queue.empty():\n",
        "        # Aggressively clear memory before processing\n",
        "        import gc\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Get batch of requests\n",
        "        batch = []\n",
        "        batch_ids = []\n",
        "        for _ in range(min(config.batch_size, request_queue.qsize())):\n",
        "            if not request_queue.empty():\n",
        "                req = request_queue.get()\n",
        "                batch.append(req.prompt)\n",
        "                batch_ids.append(req.id)\n",
        "                req.start_time = time.time()\n",
        "                results[req.id] = req\n",
        "\n",
        "        if not batch:\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            # Tokenize inputs with padding to 128 token multiples for optimal memory alignment\n",
        "            batch_inputs = local_tokenizer(\n",
        "                batch,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=\"max_length\",\n",
        "                max_length=config.max_input_length,\n",
        "                truncation=True,\n",
        "                pad_to_multiple_of=128  # Pass as parameter for memory alignment\n",
        "            ).to(device)\n",
        "\n",
        "            input_lengths = []\n",
        "            for prompt in batch:\n",
        "                tokens = local_tokenizer.encode(prompt)\n",
        "                input_lengths.append(len(tokens))\n",
        "\n",
        "            # Generate outputs with optimized settings\n",
        "            with torch.no_grad():\n",
        "                start_time = time.time()\n",
        "\n",
        "                # Use static cache with sliding window optimizations\n",
        "                generation_kwargs = {\n",
        "                    \"max_new_tokens\": config.max_output_length,\n",
        "                    \"do_sample\": False,  # Deterministic for benchmark\n",
        "                    \"use_cache\": True,\n",
        "                    \"cache_implementation\": \"static\"\n",
        "                }\n",
        "\n",
        "                # Generate with optimized settings\n",
        "                outputs = model.generate(\n",
        "                    **batch_inputs,\n",
        "                    **generation_kwargs\n",
        "                )\n",
        "\n",
        "                end_time = time.time()\n",
        "                print(f\"Generation took {end_time - start_time:.2f} seconds\")\n",
        "                print(f\"Memory after generation: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "\n",
        "            # Process results\n",
        "            for i, (output, req_id, input_length) in enumerate(zip(outputs, batch_ids, input_lengths)):\n",
        "                output_text = local_tokenizer.decode(output, skip_special_tokens=True)\n",
        "                output_tokens = len(local_tokenizer.encode(output_text))\n",
        "                output_length = output_tokens - input_length if output_tokens > input_length else 0\n",
        "\n",
        "                # Calculate metrics\n",
        "                total_tokens = input_length + output_length\n",
        "                elapsed_time = end_time - start_time\n",
        "                throughput = total_tokens / elapsed_time\n",
        "\n",
        "                # Update result\n",
        "                req = results[req_id]\n",
        "                req.result = output_text\n",
        "                req.end_time = time.time()\n",
        "                req.metrics = {\n",
        "                    \"input_tokens\": input_length,\n",
        "                    \"output_tokens\": output_length,\n",
        "                    \"total_tokens\": total_tokens,\n",
        "                    \"generation_time\": elapsed_time,\n",
        "                    \"throughput\": throughput\n",
        "                }\n",
        "\n",
        "            # Explicitly clear CUDA cache after each batch\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"Memory after cache clear: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing batch: {e}\")\n",
        "            # Update results with error info\n",
        "            for req_id in batch_ids:\n",
        "                req = results[req_id]\n",
        "                if req and not req.metrics:\n",
        "                    req.end_time = time.time()\n",
        "                    req.metrics = {\n",
        "                        \"input_tokens\": 0,\n",
        "                        \"output_tokens\": 0,\n",
        "                        \"total_tokens\": 0,\n",
        "                        \"generation_time\": 0,\n",
        "                        \"throughput\": 0,\n",
        "                        \"error\": str(e)\n",
        "                    }"
      ],
      "metadata": {
        "id": "BRmMdoJeYECf"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmark Implementation\n",
        "# Run comprehensive benchmarks to measure throughput against 200 tokens/sec target\n",
        "# Creates optimized prompts and measures aggregate performance"
      ],
      "metadata": {
        "id": "DfoIwQql-MZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_benchmark(model, tokenizer, config: InferenceConfig):\n",
        "    \"\"\"\n",
        "    Run a benchmark with 32 concurrent requests to verify 200+ tokens/sec throughput.\n",
        "    Uses exact 128-token inputs and static KV cache for maximum performance.\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Running Benchmark ===\")\n",
        "\n",
        "    # Create exactly 128-token prompt for optimal memory usage\n",
        "    base_prompt = \"Explain the differences between transformer models like BERT, GPT, and T5. Include details about their architecture, training objectives, and typical use cases.\"\n",
        "    # Pad the prompt with additional text to reach exactly 128 tokens\n",
        "    padding_text = \" This analysis should cover model size, parameter count, and computational requirements.\"\n",
        "\n",
        "    # Calibrate to exactly 128 tokens\n",
        "    while len(tokenizer.encode(base_prompt)) < 128:\n",
        "        base_prompt += padding_text\n",
        "\n",
        "    # Trim if necessary\n",
        "    encoded_prompt = tokenizer.encode(base_prompt)\n",
        "    if len(encoded_prompt) > 128:\n",
        "        encoded_prompt = encoded_prompt[:128]\n",
        "        base_prompt = tokenizer.decode(encoded_prompt)\n",
        "\n",
        "    # Verify we have exactly 128 tokens\n",
        "    final_length = len(tokenizer.encode(base_prompt))\n",
        "    print(f\"Optimized prompt token length: {final_length}\")\n",
        "\n",
        "    # Create exactly 32 requests with our optimized prompt\n",
        "    prompts = [base_prompt] * 32\n",
        "\n",
        "    # Configure static KV cache for better performance\n",
        "    try:\n",
        "        model.config.use_cache = True\n",
        "        print(\"Enabled model KV cache\")\n",
        "    except:\n",
        "        print(\"Could not explicitly enable KV cache (may already be enabled)\")\n",
        "\n",
        "    # Run inference\n",
        "    start_time = time.time()\n",
        "    results = run_concurrent_inference(model, tokenizer, prompts, config)\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate and print metrics\n",
        "    metrics = calculate_aggregate_metrics(results)\n",
        "\n",
        "    print(\"\\n=== Benchmark Results ===\")\n",
        "    print(f\"Total requests: {metrics.get('total_requests', len(results))}\")\n",
        "    print(f\"Successful requests: {metrics.get('successful_requests', 0)}\")\n",
        "    print(f\"Total tokens processed: {metrics.get('total_tokens', 0)}\")\n",
        "    print(f\"Total time: {metrics.get('total_time_seconds', 0):.2f} seconds\")\n",
        "    print(f\"Aggregate throughput: {metrics.get('aggregate_throughput', 0):.2f} tokens/sec\")\n",
        "    print(f\"Average latency per request: {metrics.get('average_latency', 0):.2f} seconds\")\n",
        "    print(f\"Target throughput: 200 tokens/sec\")\n",
        "\n",
        "    if metrics.get('aggregate_throughput', 0) >= 200:\n",
        "        print(\"✅ BENCHMARK PASSED: Throughput meets or exceeds target\")\n",
        "    else:\n",
        "        print(\"❌ BENCHMARK FAILED: Throughput below target\")\n",
        "        print(\"Trying to optimize...\")\n",
        "\n",
        "    return metrics, results"
      ],
      "metadata": {
        "id": "Mq8EVY6NYJKU"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concurrent Inference Engine\n",
        "# Implements optimized inference with T4-specific batching strategies\n",
        "# Uses a single worker with large batch size for maximum throughput"
      ],
      "metadata": {
        "id": "H-Uub_FYEQ_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_concurrent_inference(model, tokenizer, prompts: List[str], config: InferenceConfig):\n",
        "    \"\"\"\n",
        "    Run inference with optimized batching based on T4-specific performance findings.\n",
        "    Counterintuitively, a single worker with large batch size outperforms multiple workers.\n",
        "    \"\"\"\n",
        "    request_queue = Queue()\n",
        "    results = [None] * len(prompts)\n",
        "\n",
        "    # Populate request queue\n",
        "    for i, prompt in enumerate(prompts):\n",
        "        request_queue.put(InferenceRequest(id=i, prompt=prompt))\n",
        "\n",
        "    # Use a single worker as our testing showed higher throughput\n",
        "    num_workers = 1\n",
        "    print(f\"Starting {num_workers} worker thread with batch size {config.batch_size}\")\n",
        "\n",
        "    # Create and start worker thread\n",
        "    worker = Thread(\n",
        "        target=process_requests,\n",
        "        args=(model, tokenizer, request_queue, results, config)\n",
        "    )\n",
        "    worker.start()\n",
        "\n",
        "    # Wait for worker to finish\n",
        "    worker.join()\n",
        "\n",
        "    return results\n",
        "\n",
        "def calculate_aggregate_metrics(results: List[InferenceRequest]):\n",
        "    \"\"\"\n",
        "    Calculate aggregate metrics across all requests.\n",
        "    \"\"\"\n",
        "    # Filter out None values\n",
        "    valid_results = [r for r in results if r is not None and r.metrics is not None]\n",
        "\n",
        "    if not valid_results:\n",
        "        return {\n",
        "            \"total_input_tokens\": 0,\n",
        "            \"total_output_tokens\": 0,\n",
        "            \"total_tokens\": 0,\n",
        "            \"total_time_seconds\": 0,\n",
        "            \"aggregate_throughput\": 0,\n",
        "            \"average_latency\": 0,\n",
        "            \"error\": \"No valid results to calculate metrics\"\n",
        "        }\n",
        "\n",
        "    total_input_tokens = sum(r.metrics.get(\"input_tokens\", 0) for r in valid_results)\n",
        "    total_output_tokens = sum(r.metrics.get(\"output_tokens\", 0) for r in valid_results)\n",
        "    total_tokens = total_input_tokens + total_output_tokens\n",
        "\n",
        "    # Calculate end-to-end time (from first request start to last request end)\n",
        "    start_time = min(r.start_time for r in valid_results)\n",
        "    end_time = max(r.end_time for r in valid_results)\n",
        "    total_time = end_time - start_time\n",
        "\n",
        "    # Calculate aggregate throughput\n",
        "    aggregate_throughput = total_tokens / total_time if total_time > 0 else 0\n",
        "\n",
        "    # Calculate average latency\n",
        "    latencies = [(r.end_time - r.start_time) for r in valid_results]\n",
        "    avg_latency = sum(latencies) / len(latencies) if latencies else 0\n",
        "\n",
        "    return {\n",
        "        \"total_input_tokens\": total_input_tokens,\n",
        "        \"total_output_tokens\": total_output_tokens,\n",
        "        \"total_tokens\": total_tokens,\n",
        "        \"total_time_seconds\": total_time,\n",
        "        \"aggregate_throughput\": aggregate_throughput,\n",
        "        \"average_latency\": avg_latency,\n",
        "        \"successful_requests\": len(valid_results),\n",
        "        \"total_requests\": len(results)\n",
        "    }"
      ],
      "metadata": {
        "id": "EVzaxRBydfaK"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimal Configuration\n",
        "# Settings derived from extensive research and testing on T4 GPUs\n",
        "# Provides baseline configuration for maximum throughput"
      ],
      "metadata": {
        "id": "_evd8-2gEdLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def configure_optimal_settings():\n",
        "    \"\"\"\n",
        "    Configure optimal settings for the T4 GPU based on testing.\n",
        "    Returns an InferenceConfig with optimal settings.\n",
        "    \"\"\"\n",
        "    config = InferenceConfig(\n",
        "        model_id=\"mistralai/Mistral-7B-v0.1\",\n",
        "        dtype=torch.float16,\n",
        "        max_input_length=128,\n",
        "        max_output_length=128,\n",
        "        batch_size=8,  # Adjusted based on memory constraints\n",
        "    )\n",
        "\n",
        "    # Set KV cache optimization\n",
        "    config.cache_implementation = \"quantized\"  # Change from static to quantized\n",
        "    config.kv_cache_precision = \"int8\"  # Use 8-bit quantization for KV cache\n",
        "\n",
        "    # Set cache_config for quantized implementation\n",
        "    config.cache_config = {\n",
        "        \"axis_key\": 1,  # Changed from axis-key to axis_key\n",
        "        \"axis_value\": 1,  # Changed from axis-value to axis_value\n",
        "        \"backend\": \"hqq\",\n",
        "        \"bits\": 8  # INT8 quantization\n",
        "    }\n",
        "\n",
        "    # Enable Flash Attention if available\n",
        "    config.use_flash_attention = True\n",
        "    config.use_sdpa = True\n",
        "\n",
        "    # Enable memory access optimizations\n",
        "    config.optimize_memory_access = True\n",
        "    config.prefetch_kv_cache = True\n",
        "\n",
        "    return config"
      ],
      "metadata": {
        "id": "_IQcvYQus2gI"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallelism Strategy\n",
        "# Implements T4-specific thread management and CUDA optimizations\n",
        "# Focuses on optimal thread count and memory utilization"
      ],
      "metadata": {
        "id": "6lXlXKMYEf5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_parallelism():\n",
        "    \"\"\"\n",
        "    Optimize for parallelism on T4 to improve throughput.\n",
        "    This function configures the model to better handle concurrent requests.\n",
        "    \"\"\"\n",
        "    # Optimize CUDA operations\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # Set optimal thread count for CPU operations\n",
        "    import os\n",
        "    num_threads = min(32, os.cpu_count() or 8)\n",
        "    torch.set_num_threads(num_threads)\n",
        "    print(f\"Set thread count to {num_threads}\")\n",
        "\n",
        "    # Optimize memory allocation\n",
        "    from torch.utils.checkpoint import checkpoint_sequential\n",
        "    print(\"Enabled optimized memory allocation\")\n",
        "\n",
        "    # Return the optimal batch size based on memory testing\n",
        "    return 16  # This is a starting point, can be adjusted"
      ],
      "metadata": {
        "id": "aAq3APDrhBs5"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Authentication and Package Setup\n",
        "# Set up Hugging Face authentication and ensure required packages are available"
      ],
      "metadata": {
        "id": "ZzcALUE3Eh5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(\"your_hf_token\")"
      ],
      "metadata": {
        "id": "ZByOoslxYxC8"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxh5uuTkZJyl",
        "outputId": "ff4f20cf-9ddd-4823-d8e2-8e04308e3448"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.4)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U transformers accelerate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s36RSeTfZnqD",
        "outputId": "3846d79b-a62f-401f-f87c-ab1cc22b8a63"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U flash-attn --no-build-isolation\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tokQ1Yw4ag5_",
        "outputId": "ac76eebd-b45d-4c1e-a590-7d37be7c5d8d"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flash-attn in /usr/local/lib/python3.11/dist-packages (2.7.4.post1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn) (2.6.0+cu124)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate safetensors optimum einops flash-attn"
      ],
      "metadata": {
        "id": "4kzWhgB-ncdv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate safetensors optimum einops flash-attn bitsandbytes"
      ],
      "metadata": {
        "id": "Hv5CNf5wnps0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## T4-Specific Optimizations\n",
        "# Hardware-specific initialization and cache implementation testing\n",
        "# Configures memory layout and TF32 precision settings"
      ],
      "metadata": {
        "id": "z70158asEqMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_for_t4():\n",
        "    \"\"\"\n",
        "    Apply T4-specific optimizations\n",
        "    \"\"\"\n",
        "    # Enable TF32 for faster matrix multiplications on T4\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    # Set benchmark mode for faster performance after initial overhead\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # Use cusolver instead of cublas - fix for the error\n",
        "    torch.backends.cuda.preferred_linalg_library(\"cusolver\")  # Use cusolver instead of cublas\n",
        "\n",
        "    # Set environment variables for better GPU utilization\n",
        "    os.environ[\"CUDA_AUTO_TUNE\"] = \"1\"\n",
        "\n",
        "    print(\"Applied T4-specific optimizations\")"
      ],
      "metadata": {
        "id": "E7kaoEVrjwnM"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q peft"
      ],
      "metadata": {
        "id": "a99KVRiYcCPl"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Throughput Maximization\n",
        "# Implements memory bandwidth and utilization optimizations\n",
        "# Monitors GPU memory usage during inference"
      ],
      "metadata": {
        "id": "qgXWd1rWExei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def monitor_gpu_memory():\n",
        "    \"\"\"\n",
        "    Monitor GPU memory usage\n",
        "    \"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        return None\n",
        "\n",
        "    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB\n",
        "    reserved_memory = torch.cuda.memory_reserved(0) / (1024**3)  # GB\n",
        "    allocated_memory = torch.cuda.memory_allocated(0) / (1024**3)  # GB\n",
        "    free_memory = total_memory - reserved_memory\n",
        "\n",
        "    print(f\"GPU Memory: {allocated_memory:.2f}GB allocated, {reserved_memory:.2f}GB reserved, {free_memory:.2f}GB free of {total_memory:.2f}GB total\")\n",
        "\n",
        "    return {\n",
        "        \"total_gb\": total_memory,\n",
        "        \"reserved_gb\": reserved_memory,\n",
        "        \"allocated_gb\": allocated_memory,\n",
        "        \"free_gb\": free_memory\n",
        "    }"
      ],
      "metadata": {
        "id": "avf_1-5okDm1"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_cache_implementations(model, tokenizer, prompt=\"Explain quantum computing in simple terms\"):\n",
        "    \"\"\"\n",
        "    Test different cache implementations to find the optimal one.\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Testing Cache Implementations ===\")\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    implementations = {\n",
        "        \"default\": {\"cache_implementation\": None, \"cache_config\": None},\n",
        "        \"static\": {\"cache_implementation\": \"static\", \"cache_config\": None},\n",
        "        \"quantized_int8\": {\n",
        "            \"cache_implementation\": \"quantized\",\n",
        "            \"cache_config\": {\"axis-key\": 1, \"axis-value\": 1, \"backend\": \"hqq\"}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for name, config in implementations.items():\n",
        "        print(f\"\\nTesting {name} cache...\")\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Warmup\n",
        "        with torch.no_grad():\n",
        "            try:\n",
        "                for _ in range(2):\n",
        "                    with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n",
        "                        _ = model.generate(\n",
        "                            **inputs,\n",
        "                            max_new_tokens=32,\n",
        "                            do_sample=False,\n",
        "                            cache_implementation=config[\"cache_implementation\"],\n",
        "                            cache_config=config[\"cache_config\"]\n",
        "                        )\n",
        "            except Exception as e:\n",
        "                print(f\"Error during warmup: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Timed run\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            with torch.no_grad():\n",
        "                with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n",
        "                    outputs = model.generate(\n",
        "                        **inputs,\n",
        "                        max_new_tokens=128,\n",
        "                        do_sample=False,\n",
        "                        cache_implementation=config[\"cache_implementation\"],\n",
        "                        cache_config=config[\"cache_config\"]\n",
        "                    )\n",
        "            end_time = time.time()\n",
        "\n",
        "            tokens = len(outputs[0])\n",
        "            elapsed = end_time - start_time\n",
        "            throughput = tokens / elapsed\n",
        "\n",
        "            memory_used = torch.cuda.max_memory_allocated() / 1024**3  # Convert to GB\n",
        "\n",
        "            results[name] = {\n",
        "                \"throughput\": throughput,\n",
        "                \"memory_used_gb\": memory_used,\n",
        "                \"time_seconds\": elapsed\n",
        "            }\n",
        "\n",
        "            print(f\"Tokens: {tokens}\")\n",
        "            print(f\"Time: {elapsed:.2f} seconds\")\n",
        "            print(f\"Throughput: {throughput:.2f} tokens/sec\")\n",
        "            print(f\"Memory used: {memory_used:.2f} GB\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "    # Find the best implementation\n",
        "    if results:\n",
        "        best_impl = max(results.items(), key=lambda x: x[1][\"throughput\"])\n",
        "        print(f\"\\nBest implementation: {best_impl[0]} with {best_impl[1]['throughput']:.2f} tokens/sec\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "D7hV_4_2eebE"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_throughput(model, tokenizer, config):\n",
        "    \"\"\"\n",
        "    Apply targeted optimizations to maximize throughput\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Applying Throughput Optimizations ===\")\n",
        "\n",
        "    # 1. Enable KV cache optimizations\n",
        "    try:\n",
        "        model.config.use_cache = True\n",
        "        print(\"Enabled KV cache\")\n",
        "    except:\n",
        "        print(\"Could not modify model config\")\n",
        "\n",
        "    # 2. Increase concurrent processing\n",
        "    starting_batch_size = config.batch_size\n",
        "    print(f\"Starting with batch size {starting_batch_size}\")\n",
        "\n",
        "    # Try different batch sizes\n",
        "    batch_sizes = [starting_batch_size, min(starting_batch_size * 2, 32)]\n",
        "    results = {}\n",
        "\n",
        "    for batch_size in batch_sizes:\n",
        "        print(f\"\\nTesting batch size {batch_size}...\")\n",
        "        config.batch_size = batch_size\n",
        "\n",
        "        # Run quick benchmark\n",
        "        sample_prompt = \"Explain quantum computing in simple terms\" * 2\n",
        "        prompts = [sample_prompt] * 16  # Use fewer for faster testing\n",
        "\n",
        "        # Create request objects\n",
        "        request_queue = Queue()\n",
        "        results_list = [None] * len(prompts)\n",
        "        for i, prompt in enumerate(prompts):\n",
        "            request_queue.put(InferenceRequest(id=i, prompt=prompt))\n",
        "\n",
        "        # Calculate optimal worker threads\n",
        "        num_workers = max(1, min(4, 16 // batch_size))\n",
        "\n",
        "        # Reset memory\n",
        "        torch.cuda.empty_cache()\n",
        "        monitor_gpu_memory()\n",
        "\n",
        "        # Create and start worker threads\n",
        "        workers = []\n",
        "        for _ in range(num_workers):\n",
        "            worker = Thread(\n",
        "                target=process_requests,\n",
        "                args=(model, tokenizer, request_queue, results_list, config)\n",
        "            )\n",
        "            workers.append(worker)\n",
        "            worker.start()\n",
        "\n",
        "        # Wait for all workers to finish\n",
        "        for worker in workers:\n",
        "            worker.join()\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = calculate_aggregate_metrics(results_list)\n",
        "        results[batch_size] = metrics.get('aggregate_throughput', 0)\n",
        "\n",
        "        print(f\"Batch size {batch_size}: {results[batch_size]:.2f} tokens/sec\")\n",
        "\n",
        "    # Find the best batch size\n",
        "    best_batch_size = max(results.items(), key=lambda x: x[1])[0]\n",
        "    print(f\"\\nOptimal batch size: {best_batch_size} with {results[best_batch_size]:.2f} tokens/sec\")\n",
        "\n",
        "    # Set the optimal batch size\n",
        "    config.batch_size = best_batch_size\n",
        "\n",
        "    # Try with static cache if available\n",
        "    try:\n",
        "        # Test with small prompt\n",
        "        inputs = tokenizer(\"Test prompt\", return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=10,\n",
        "                cache_implementation=\"static\"\n",
        "            )\n",
        "        print(\"Static KV cache is available and will be used\")\n",
        "        config.use_static_cache = True\n",
        "    except Exception as e:\n",
        "        print(f\"Static KV cache not supported: {e}\")\n",
        "        config.use_static_cache = False\n",
        "\n",
        "    print(\"\\nOptimization complete\")\n",
        "    return config"
      ],
      "metadata": {
        "id": "zYaYPparkErn"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_for_high_throughput():\n",
        "    \"\"\"\n",
        "    Apply specific initializations for maximum T4 throughput\n",
        "    \"\"\"\n",
        "    # Enable TF32 for math operations\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    # Optimize CUDA operations\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # Memory optimization\n",
        "    import gc\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # T4-specific memory allocation strategy - optimal for large batches\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
        "\n",
        "    # Use fewer threads for better thread efficiency\n",
        "    torch.set_num_threads(4)\n",
        "\n",
        "    # Configure CUDA graphs for faster kernel launches\n",
        "    try:\n",
        "        torch._C._jit_set_nvfuser_enabled(True)\n",
        "        print(\"CUDA graph optimization enabled\")\n",
        "    except:\n",
        "        print(\"Could not enable CUDA graph optimization\")\n",
        "\n",
        "    print(\"Applied high-throughput optimizations for T4\")\n",
        "\n",
        "    # Return optimal batch size - this matched our testing\n",
        "    return 32  # Our testing found this to be optimal"
      ],
      "metadata": {
        "id": "ldLNNJxC-WjE"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Application\n",
        "# Combines all optimizations for maximum inference throughput\n",
        "# Implements benchmarking and interactive mode for testing"
      ],
      "metadata": {
        "id": "6p-6RF2nrUfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "      \"\"\"\n",
        "    Main application combining all optimization techniques to achieve 200+ tokens/sec.\n",
        "    Implements both benchmark mode for throughput verification and interactive mode for testing.\n",
        "    \"\"\"\n",
        "    # Apply T4-specific initializations\n",
        "    optimal_batch_size = initialize_for_high_throughput()\n",
        "\n",
        "    # Configuration with research-based optimizations\n",
        "    config = InferenceConfig(\n",
        "        model_id=\"mistralai/Mistral-7B-v0.1\",\n",
        "        dtype=torch.float16,\n",
        "        max_input_length=128,\n",
        "        max_output_length=128,\n",
        "        batch_size=optimal_batch_size,   # From T4-specific optimization research\n",
        "        # KV cache settings with optimizations\n",
        "        cache_implementation=\"static\",\n",
        "        # Disable incompatible optimizations\n",
        "        use_flash_attention=False,  # Not compatible with T4 architecture\n",
        "        use_sdpa=False,             # Not supported on compute capability 7.5\n",
        "        memory_efficient=True       # Enable memory optimizations\n",
        "    )\n",
        "\n",
        "    # Update model_id if provided\n",
        "    model_id = input(\"Enter model ID (default: mistralai/Mistral-7B-v0.1): \").strip()\n",
        "    if model_id:\n",
        "        config.model_id = model_id\n",
        "\n",
        "    # Ask if using LoRA\n",
        "    use_lora = input(\"Use LoRA model? (y/n, default: n): \").strip().lower()\n",
        "    if use_lora == 'y':\n",
        "        config.use_lora = True\n",
        "        config.lora_path = input(\"Enter LoRA adapter path: \").strip()\n",
        "\n",
        "    # Load and optimize model with advanced techniques\n",
        "    model, tokenizer = load_and_optimize_model(config)\n",
        "\n",
        "    # Warm up the model\n",
        "    warmup_model(model, tokenizer, config)\n",
        "\n",
        "    # Monitor memory before benchmark\n",
        "    print(\"\\nMemory before benchmark:\")\n",
        "    print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "    print(f\"Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
        "\n",
        "    # Run benchmark\n",
        "    metrics, _ = run_benchmark(model, tokenizer, config)\n",
        "\n",
        "    # Interactive mode\n",
        "    print(\"\\n=== Interactive Mode ===\")\n",
        "    print(\"Type 'exit' to quit\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nEnter prompt: \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        # Process single request\n",
        "        start_time = time.time()\n",
        "        # Use 128-token padding for alignment\n",
        "        inputs = tokenizer(\n",
        "            user_input,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            pad_to_multiple_of=128\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Use optimized generation settings\n",
        "            generation_kwargs = {\n",
        "                \"max_new_tokens\": config.max_output_length,\n",
        "                \"do_sample\": True,  # Allow sampling for more natural responses in interactive mode\n",
        "                \"temperature\": 0.7,\n",
        "                \"top_p\": 0.9,\n",
        "                \"cache_implementation\": \"static\"\n",
        "            }\n",
        "\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                **generation_kwargs\n",
        "            )\n",
        "\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        end_time = time.time()\n",
        "\n",
        "        # Calculate metrics\n",
        "        input_tokens = len(tokenizer.encode(user_input))\n",
        "        output_tokens = len(tokenizer.encode(response)) - input_tokens\n",
        "        total_tokens = input_tokens + output_tokens\n",
        "        inference_time = end_time - start_time\n",
        "        throughput = total_tokens / inference_time\n",
        "\n",
        "        print(\"\\nResponse:\")\n",
        "        print(response)\n",
        "        print(\"\\nPerformance Metrics:\")\n",
        "        print(f\"Input tokens: {input_tokens}\")\n",
        "        print(f\"Output tokens: {output_tokens}\")\n",
        "        print(f\"Inference time: {inference_time:.2f} seconds\")\n",
        "        print(f\"Throughput: {throughput:.2f} tokens/sec\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "1vVfe3_yYckB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "937d6f897d6246258843cc2432debaca",
            "8fadfb9a9b934886a3455dff3d75bc18",
            "470826472d434ec1a319ef2e609f32bb",
            "139d9dc7d4454ad395b1b52e49f65b26",
            "c0c60b857e2e43369fccbfd789dd8480",
            "537ce7e6a65a497cae61f33a902897c8",
            "f3db47d8aedf42c0a48c21641f4d5409",
            "30bdba5048fa4ce883a38b1726781ef9",
            "6da71bdc3e63481194afd11975f43194",
            "1bcaf0839d954fa69b5e7eb42da6748d",
            "c363ce43192343d8b030b816f54cef9f"
          ]
        },
        "outputId": "8db2ac78-1121-4072-99b2-425f9f0b87b3"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA graph optimization enabled\n",
            "Applied high-throughput optimizations for T4\n",
            "Enter model ID (default: mistralai/Mistral-7B-v0.1): mistralai/Mistral-7B-v0.1\n",
            "Use LoRA model? (y/n, default: n): y\n",
            "Enter LoRA adapter path: teknium/OpenHermes-2.5-Mistral-7B\n",
            "Loading model: mistralai/Mistral-7B-v0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "937d6f897d6246258843cc2432debaca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying torch.compile with max-autotune...\n",
            "Compilation successful\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 128-token padding alignment for optimal memory access\n",
            "Model loaded and optimized in 68.53 seconds\n",
            "Starting model warmup...\n",
            "Warmup with batch size 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warmup with batch size 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warmup with batch size 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running final warmup with maximum batch size\n",
            "Warmup completed in 224.90 seconds\n",
            "\n",
            "Memory before benchmark:\n",
            "Allocated: 7.70 GB\n",
            "Reserved: 9.17 GB\n",
            "\n",
            "=== Running Benchmark ===\n",
            "Optimized prompt token length: 129\n",
            "Enabled model KV cache\n",
            "Starting 1 worker thread with batch size 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial GPU memory: 7.70 GB allocated\n",
            "Generation took 38.03 seconds\n",
            "Memory after generation: 8.70 GB\n",
            "Memory after cache clear: 8.70 GB\n",
            "\n",
            "=== Benchmark Results ===\n",
            "Total requests: 32\n",
            "Successful requests: 32\n",
            "Total tokens processed: 8160\n",
            "Total time: 38.09 seconds\n",
            "Aggregate throughput: 214.25 tokens/sec\n",
            "Average latency per request: 38.07 seconds\n",
            "Target throughput: 200 tokens/sec\n",
            "✅ BENCHMARK PASSED: Throughput meets or exceeds target\n",
            "\n",
            "=== Interactive Mode ===\n",
            "Type 'exit' to quit\n",
            "\n",
            "Enter prompt: what is life from the pov of a fly\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Response:\n",
            "what is life from the pov of a fly\n",
            "\n",
            "the whole world is a huge fly swatter\n",
            "\n",
            "and the fly is the one swatting\n",
            "\n",
            "but the fly is just a tiny part of the swatter\n",
            "\n",
            "and the swatter is just a tiny part of the world\n",
            "\n",
            "and the world is just a tiny part of the universe\n",
            "\n",
            "and the universe is just a tiny part of infinity\n",
            "\n",
            "and infinity is just a tiny part of god\n",
            "\n",
            "and god is just a tiny part of the fly\n",
            "\n",
            "and the fly is just a tiny part of the swatter\n",
            "\n",
            "and the swatter is just a tiny part of the world\n",
            "\n",
            "\n",
            "Performance Metrics:\n",
            "Input tokens: 10\n",
            "Output tokens: 128\n",
            "Inference time: 8.28 seconds\n",
            "Throughput: 16.67 tokens/sec\n",
            "\n",
            "Enter prompt: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rdxUTH89yCBx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}