{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f485a650dc8c44bca56cf437d48f86a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_04be03772ee246ffbbd506ce265e9055",
              "IPY_MODEL_b29cb7127b4c4938845ea6fe7e94934e",
              "IPY_MODEL_d36b6683fcf64b0cb5514ca21f2b94d4"
            ],
            "layout": "IPY_MODEL_b2c8b20a018a432a81191e3958a896df"
          }
        },
        "04be03772ee246ffbbd506ce265e9055": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cd8b6bf57cb40fc9c92b638c6a12aa0",
            "placeholder": "​",
            "style": "IPY_MODEL_ad8c084d382e4d5db8f08660355f62fa",
            "value": ""
          }
        },
        "b29cb7127b4c4938845ea6fe7e94934e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d02f81d1937a474ab951c0d4a2633bf9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_63300c4defbc4b2095b221c2228eaa0f",
            "value": 1
          }
        },
        "d36b6683fcf64b0cb5514ca21f2b94d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3800e330caab45ceb46e6879e30a7b03",
            "placeholder": "​",
            "style": "IPY_MODEL_7642958c8f1e41d5bae746f34e1ce2c5",
            "value": "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11&lt;00:00, 11.39s/it]\n"
          }
        },
        "b2c8b20a018a432a81191e3958a896df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cd8b6bf57cb40fc9c92b638c6a12aa0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad8c084d382e4d5db8f08660355f62fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d02f81d1937a474ab951c0d4a2633bf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63300c4defbc4b2095b221c2228eaa0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3800e330caab45ceb46e6879e30a7b03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7642958c8f1e41d5bae746f34e1ce2c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3NvRPbOX6mE"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate safetensors einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate safetensors optimum einops flash-attn bitsandbytes"
      ],
      "metadata": {
        "id": "1vEdj3iudQXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n"
      ],
      "metadata": {
        "id": "raQyQAYs-XWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "IyTcmYVOrrwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q hqq transformers[torch] optimum"
      ],
      "metadata": {
        "id": "zOSGztzpkhnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell to Add: Install vLLM\n",
        "!pip install vllm -q\n",
        "# Note: Depending on the Colab environment's CUDA/Driver version, you might need a specific vLLM build.\n",
        "# If the above fails, check vLLM documentation for compatibility."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDYNCDdpts2L",
        "outputId": "522fe0e7-0693-4b0e-dc99-5ab22134e075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.0/294.0 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.3/343.3 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.0/239.0 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m913.7/913.7 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.0 which is incompatible.\n",
            "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.0 which is incompatible.\n",
            "distributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.0 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install necessary libraries\n",
        "# We primarily need vLLM and transformers (for the tokenizer)\n",
        "!pip install vllm==0.5.1  # Specify a version for potentially better stability\n",
        "!pip install transformers==4.42.4 -q # Keep transformers version consistent\n",
        "print(\"--- Installation Complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTVZ3om_vf2w",
        "outputId": "c3a56b47-53aa-4e07-d86d-0bda7cfd09c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: vllm==0.5.1 in /usr/local/lib/python3.11/dist-packages (0.5.1)\n",
            "Requirement already satisfied: cmake>=3.21 in /usr/local/lib/python3.11/dist-packages (from vllm==0.5.1) (3.31.6)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from vllm==0.5.1) (1.11.1.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from vllm==0.5.1) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from vllm==0.5.1) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.5.1) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from vllm==0.5.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from vllm==0.5.1) (4.67.1)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm==0.5.1) (9.0.0)\n",
            "Collecting transformers>=4.42.0 (from vllm==0.5.1)\n",
            "  Using cached transformers-4.51.0-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from vllm==0.5.1) (0.19.1)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from vllm==0.5.1) (0.115.12)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm==0.5.1) (3.11.15)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from vllm==0.5.1) (1.70.0)\n",
            "Requirement already satisfied: uvicorn[standard] in /usr/local/lib/python3.11/dist-packages (from vllm==0.5.1) (0.34.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.5.1) (2.11.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm==0.5.1) (11.1.0)\n",
            "Requirement already satisfied: prometheus-client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.5.1) (0.21.1)\n",
            "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.5.1) (7.1.0)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.5.1) (0.9.0)\n",
            "Requirement already satisfied: lm-format-enforcer==0.10.1 in /usr/local/lib/python3.11/dist-packages (from vllm==0.5.1) (0.10.1)\n",
            "Requirement already satisfied: outlines>=0.0.43 in /usr/local/lib/python3.11/dist-packages (from vllm==0.5.1) (0.1.11)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from vllm==0.5.1) (4.13.1)\n",
            "Requirement already satisfied: filelock>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from vllm==0.5.1) (3.18.0)\n",
            "Requirement already satisfied: ray>=2.9 in /usr/local/lib/python3.11/dist-packages (from vllm==0.5.1) (2.43.0)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.11/dist-packages (from vllm==0.5.1) (12.570.86)\n",
            "Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.5.1) (2.3.0)\n",
            "Requirement already satisfied: torchvision==0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.5.1) (0.18.0)\n",
            "Requirement already satisfied: xformers==0.0.26.post1 in /usr/local/lib/python3.11/dist-packages (from vllm==0.5.1) (0.0.26.post1)\n",
            "Requirement already satisfied: vllm-flash-attn==2.5.9 in /usr/local/lib/python3.11/dist-packages (from vllm==0.5.1) (2.5.9)\n",
            "Requirement already satisfied: interegular>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from lm-format-enforcer==0.10.1->vllm==0.5.1) (0.3.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lm-format-enforcer==0.10.1->vllm==0.5.1) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from lm-format-enforcer==0.10.1->vllm==0.5.1) (6.0.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0->vllm==0.5.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0->vllm==0.5.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0->vllm==0.5.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0->vllm==0.5.1) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0->vllm==0.5.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0->vllm==0.5.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0->vllm==0.5.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0->vllm==0.5.1) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0->vllm==0.5.1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0->vllm==0.5.1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0->vllm==0.5.1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0->vllm==0.5.1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0->vllm==0.5.1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0->vllm==0.5.1) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0->vllm==0.5.1) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0->vllm==0.5.1) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->vllm==0.5.1) (12.4.127)\n",
            "Requirement already satisfied: lark in /usr/local/lib/python3.11/dist-packages (from outlines>=0.0.43->vllm==0.5.1) (1.2.2)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from outlines>=0.0.43->vllm==0.5.1) (1.6.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from outlines>=0.0.43->vllm==0.5.1) (3.1.1)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.11/dist-packages (from outlines>=0.0.43->vllm==0.5.1) (5.6.3)\n",
            "Requirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from outlines>=0.0.43->vllm==0.5.1) (0.36.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from outlines>=0.0.43->vllm==0.5.1) (4.23.0)\n",
            "Requirement already satisfied: pycountry in /usr/local/lib/python3.11/dist-packages (from outlines>=0.0.43->vllm==0.5.1) (24.6.1)\n",
            "Requirement already satisfied: airportsdata in /usr/local/lib/python3.11/dist-packages (from outlines>=0.0.43->vllm==0.5.1) (20250224)\n",
            "Requirement already satisfied: outlines_core==0.1.26 in /usr/local/lib/python3.11/dist-packages (from outlines>=0.0.43->vllm==0.5.1) (0.1.26)\n",
            "Requirement already satisfied: starlette<1.0.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from prometheus-fastapi-instrumentator>=7.0.0->vllm==0.5.1) (0.46.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->vllm==0.5.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->vllm==0.5.1) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->vllm==0.5.1) (0.4.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray>=2.9->vllm==0.5.1) (8.1.8)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray>=2.9->vllm==0.5.1) (1.1.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.11/dist-packages (from ray>=2.9->vllm==0.5.1) (5.29.4)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray>=2.9->vllm==0.5.1) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray>=2.9->vllm==0.5.1) (1.5.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.6.0->vllm==0.5.1) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->vllm==0.5.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->vllm==0.5.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->vllm==0.5.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->vllm==0.5.1) (2025.1.31)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.19.1->vllm==0.5.1) (0.30.1)\n",
            "Collecting tokenizers>=0.19.1 (from vllm==0.5.1)\n",
            "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.42.0->vllm==0.5.1) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.5.1) (2.6.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.5.1) (25.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.5.1) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.5.1) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.5.1) (1.18.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai->vllm==0.5.1) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai->vllm==0.5.1) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai->vllm==0.5.1) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai->vllm==0.5.1) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai->vllm==0.5.1) (1.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm==0.5.1) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm==0.5.1) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm==0.5.1) (1.1.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm==0.5.1) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm==0.5.1) (1.0.4)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm==0.5.1) (15.0.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->vllm==0.5.1) (1.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.3.0->vllm==0.5.1) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines>=0.0.43->vllm==0.5.1) (2024.10.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines>=0.0.43->vllm==0.5.1) (0.24.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.3.0->vllm==0.5.1) (1.3.0)\n",
            "Using cached transformers-4.51.0-py3-none-any.whl (10.4 MB)\n",
            "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.21.1 transformers\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m--- Installation Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vllm --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PDMDZRJixAHE",
        "outputId": "601a1679-9554-4e8c-9067-9d72a8c3a2a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: vllm in /usr/local/lib/python3.11/dist-packages (0.5.1)\n",
            "Collecting vllm\n",
            "  Using cached vllm-0.8.3-cp38-abi3-manylinux1_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from vllm) (5.5.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from vllm) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from vllm) (1.26.4)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from vllm) (4.67.1)\n",
            "Requirement already satisfied: blake3 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.0.4)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm) (9.0.0)\n",
            "Collecting transformers>=4.51.0 (from vllm)\n",
            "  Using cached transformers-4.51.0-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[hf_xet]>=0.30.0->vllm) (0.30.1)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.19.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from vllm) (5.29.4)\n",
            "Requirement already satisfied: fastapi>=0.115.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.115.12)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm) (3.11.15)\n",
            "Requirement already satisfied: openai>=1.52.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.70.0)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.11.2)\n",
            "Requirement already satisfied: prometheus_client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.21.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm) (11.1.0)\n",
            "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (7.1.0)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.9.0)\n",
            "Collecting lm-format-enforcer<0.11,>=0.10.11 (from vllm)\n",
            "  Using cached lm_format_enforcer-0.10.11-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: llguidance<0.8.0,>=0.7.9 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.7.13)\n",
            "Requirement already satisfied: outlines==0.1.11 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.1.11)\n",
            "Requirement already satisfied: lark==1.2.2 in /usr/local/lib/python3.11/dist-packages (from vllm) (1.2.2)\n",
            "Requirement already satisfied: xgrammar==0.1.17 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.1.17)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.13.1)\n",
            "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.11/dist-packages (from vllm) (3.18.0)\n",
            "Requirement already satisfied: partial-json-parser in /usr/local/lib/python3.11/dist-packages (from vllm) (0.2.1.1.post5)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.11/dist-packages (from vllm) (24.0.1)\n",
            "Requirement already satisfied: msgspec in /usr/local/lib/python3.11/dist-packages (from vllm) (0.19.0)\n",
            "Requirement already satisfied: gguf==0.10.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.10.0)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from vllm) (8.6.1)\n",
            "Requirement already satisfied: mistral_common>=1.5.4 in /usr/local/lib/python3.11/dist-packages (from mistral_common[opencv]>=1.5.4->vllm) (1.5.4)\n",
            "Requirement already satisfied: opencv-python-headless>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (4.11.0.86)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from vllm) (6.0.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from vllm) (0.8.1)\n",
            "Requirement already satisfied: compressed-tensors==0.9.2 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.9.2)\n",
            "Requirement already satisfied: depyf==0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.18.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from vllm) (3.1.1)\n",
            "Requirement already satisfied: watchfiles in /usr/local/lib/python3.11/dist-packages (from vllm) (1.0.4)\n",
            "Requirement already satisfied: python-json-logger in /usr/local/lib/python3.11/dist-packages (from vllm) (3.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from vllm) (1.14.1)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from vllm) (1.11.1.4)\n",
            "Requirement already satisfied: numba==0.61 in /usr/local/lib/python3.11/dist-packages (from vllm) (0.61.0)\n",
            "Requirement already satisfied: ray!=2.44.*,>=2.43.0 in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (2.43.0)\n",
            "Collecting torch==2.6.0 (from vllm)\n",
            "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: torchaudio==2.6.0 in /usr/local/lib/python3.11/dist-packages (from vllm) (2.6.0+cu124)\n",
            "Collecting torchvision==0.21.0 (from vllm)\n",
            "  Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting xformers==0.0.29.post2 (from vllm)\n",
            "  Using cached xformers-0.0.29.post2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->vllm) (0.8.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from depyf==0.18.0->vllm) (0.3.9)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba==0.61->vllm) (0.44.0)\n",
            "Requirement already satisfied: interegular in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (3.1.6)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (1.6.0)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (5.6.3)\n",
            "Requirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.36.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (4.23.0)\n",
            "Requirement already satisfied: pycountry in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (24.6.1)\n",
            "Requirement already satisfied: airportsdata in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (20250224)\n",
            "Requirement already satisfied: outlines_core==0.1.26 in /usr/local/lib/python3.11/dist-packages (from outlines==0.1.11->vllm) (0.1.26)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->vllm)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->vllm)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->vllm)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->vllm)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->vllm)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->vllm)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->vllm)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->vllm)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->vllm)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.6.0->vllm)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.6.0->vllm)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (12.4.127)\n",
            "Collecting triton==3.2.0 (from torch==2.6.0->vllm)\n",
            "  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->vllm) (1.13.1)\n",
            "Requirement already satisfied: nanobind>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from xgrammar==0.1.17->vllm) (2.6.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->vllm) (1.3.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.46.1)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.0.7)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.28.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.20)\n",
            "Requirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from fastapi[standard]>=0.115.0->vllm) (2.2.0)\n",
            "Requirement already satisfied: uvicorn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.34.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.0->huggingface-hub[hf_xet]>=0.30.0->vllm) (24.2)\n",
            "Requirement already satisfied: hf-xet>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub[hf_xet]>=0.30.0->vllm) (1.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0->vllm) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm) (0.4.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (8.1.8)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.1.0)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray!=2.44.*,>=2.43.0->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (1.5.0)\n",
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.11/dist-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm) (13.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->vllm) (2025.1.31)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.6.0->vllm) (2024.11.6)\n",
            "Collecting tokenizers>=0.19.1 (from vllm)\n",
            "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.51.0->vllm) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (2.6.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (25.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm) (1.18.3)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->vllm) (3.21.0)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.7.0)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.15.2)\n",
            "Requirement already satisfied: rich-toolkit>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.14.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->outlines==0.1.11->vllm) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (2024.10.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->outlines==0.1.11->vllm) (0.24.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.1.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.21.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (15.0.1)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x->ray[cgraph]!=2.44.*,>=2.43.0->vllm) (0.8.3)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.11/dist-packages (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (13.9.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\n",
            "Using cached vllm-0.8.3-cp38-abi3-manylinux1_x86_64.whl (294.0 MB)\n",
            "Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached xformers-0.0.29.post2-cp311-cp311-manylinux_2_28_x86_64.whl (44.3 MB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached lm_format_enforcer-0.10.11-py3-none-any.whl (44 kB)\n",
            "Using cached transformers-4.51.0-py3-none-any.whl (10.4 MB)\n",
            "Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, tokenizers, lm-format-enforcer, xformers, transformers, torchvision, vllm\n",
            "  Attempting uninstall: triton\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: triton 2.3.0\n",
            "    Uninstalling triton-2.3.0:\n",
            "      Successfully uninstalled triton-2.3.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
            "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
            "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
            "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
            "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
            "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
            "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
            "  Attempting uninstall: torch\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: torch 2.3.0\n",
            "    Uninstalling torch-2.3.0:\n",
            "      Successfully uninstalled torch-2.3.0\n",
            "  Attempting uninstall: tokenizers\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: lm-format-enforcer\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: lm-format-enforcer 0.10.1\n",
            "    Uninstalling lm-format-enforcer-0.10.1:\n",
            "      Successfully uninstalled lm-format-enforcer-0.10.1\n",
            "  Attempting uninstall: xformers\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: xformers 0.0.26.post1\n",
            "    Uninstalling xformers-0.0.26.post1:\n",
            "      Successfully uninstalled xformers-0.0.26.post1\n",
            "  Attempting uninstall: torchvision\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: torchvision 0.18.0\n",
            "    Uninstalling torchvision-0.18.0:\n",
            "      Successfully uninstalled torchvision-0.18.0\n",
            "  Attempting uninstall: vllm\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: vllm 0.5.1\n",
            "    Uninstalling vllm-0.5.1:\n",
            "      Successfully uninstalled vllm-0.5.1\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "vllm-flash-attn 2.5.9 requires torch==2.3.0, but you have torch 2.6.0 which is incompatible.\n",
            "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lm-format-enforcer-0.10.11 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvtx-cu12-12.4.127 tokenizers-0.21.1 torch-2.6.0 torchvision-0.21.0 transformers triton-3.2.0 vllm-0.8.3 xformers-0.0.29.post2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "functorch",
                  "torch",
                  "torchgen",
                  "triton",
                  "vllm"
                ]
              },
              "id": "ce7417f8bba243cb8c4d082c43ab0b33"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Imports and Configuration Setup\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import gc\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "# VLLM Imports\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "# Keep tokenizer import from transformers\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Set device (vLLM typically manages device placement, but good practice)\n",
        "device_name = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device_name}\")\n",
        "\n",
        "# Check GPU info (still useful)\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "    print(f\"GPU: {gpu_name}\")\n",
        "    print(f\"Memory: {gpu_memory:.2f} GB\")\n",
        "else:\n",
        "    print(\"No GPU detected, using CPU\")\n",
        "\n",
        "# --- Configuration Dataclasses ---\n",
        "@dataclass\n",
        "class BenchmarkConfig:\n",
        "    \"\"\"Configuration for the benchmark run.\"\"\"\n",
        "    model_id: str\n",
        "    max_input_length: int = 128\n",
        "    max_output_length: int = 128\n",
        "    num_benchmark_prompts: int = 32 # Target concurrency\n",
        "    tensor_parallel_size: int = 1 # For single T4 GPU\n",
        "    gpu_memory_utilization: float = 0.90 # vLLM: Use 90% of GPU memory for KV cache\n",
        "    quantization: Optional[str] = None # e.g., 'awq', 'gptq'. vLLM might detect automatically.\n",
        "        # --- ADD THESE TWO LINES ---\n",
        "    max_model_len_override: Optional[int] = None # Optional: Override model's max length\n",
        "    max_num_batched_tokens_override: Optional[int] = None # Optional: Override vLLM batch token limit\n",
        "\n",
        "\n",
        "    # LoRA arguments for vLLM\n",
        "    enable_lora: bool = False\n",
        "    lora_modules: Optional[List[Dict[str, Any]]] = field(default_factory=list) # Structure: [{\"lora_name\": \"...\", \"lora_int_id\": 1, \"lora_local_path\": \"...\"}]\n",
        "    max_loras: int = 1\n",
        "    max_lora_rank: int = 16 # Common default, adjust if needed for specific adapters\n",
        "\n",
        "@dataclass\n",
        "class RequestOutput:\n",
        "    \"\"\"Simple struct to hold results for each request.\"\"\"\n",
        "    id: int\n",
        "    prompt: str\n",
        "    result: str = \"\"\n",
        "    metrics: Dict[str, Any] = field(default_factory=dict)\n",
        "    start_time: float = None # Note: In vLLM batch, start/end might be approximate\n",
        "    end_time: float = None\n",
        "\n",
        "print(\"--- Imports and Config Setup Complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KNr3GixvkFx",
        "outputId": "40950994-f33d-49a7-bc79-31e2a3e93334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "Memory: 14.74 GB\n",
            "--- Imports and Config Setup Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Benchmark Function using vLLM\n",
        "def run_vllm_benchmark(tokenizer: AutoTokenizer, config: BenchmarkConfig, llm: LLM):\n",
        "    \"\"\"\n",
        "    Run a benchmark using the initialized vLLM engine.\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Running vLLM Benchmark ===\")\n",
        "    print(f\"Model: {config.model_id}\")\n",
        "    print(f\"Target Concurrency: {config.num_benchmark_prompts}\")\n",
        "    print(f\"Input/Output Length: {config.max_input_length}/{config.max_output_length}\")\n",
        "\n",
        "    # --- Prepare Benchmark Prompts ---\n",
        "    # Create a prompt designed to be exactly config.max_input_length tokens long\n",
        "    base_prompt = \"Explain the differences between transformer models like BERT, GPT, and T5. Include details about their architecture, training objectives, and typical use cases.\"\n",
        "    padding_text = \" This analysis should cover model size, parameter count, and computational requirements.\"\n",
        "\n",
        "    # Encode, truncate/pad to exact length, then decode back to ensure exact token count\n",
        "    encoded = tokenizer.encode(base_prompt)\n",
        "    if len(encoded) < config.max_input_length:\n",
        "         # Extend prompt if too short (simple repetition)\n",
        "        target_len = config.max_input_length\n",
        "        while len(encoded) < target_len:\n",
        "             encoded.extend(tokenizer.encode(padding_text, add_special_tokens=False))\n",
        "             if len(encoded) > target_len + 20: # Avoid excessive padding text\n",
        "                  break # Safety break\n",
        "    # Truncate if too long\n",
        "    if len(encoded) > config.max_input_length:\n",
        "        encoded = encoded[:config.max_input_length]\n",
        "\n",
        "    # Decode the precisely sized token list back into a string prompt\n",
        "    # Ensure skip_special_tokens=True if the tokenizer adds BOS/EOS by default in encode\n",
        "    final_prompt_string = tokenizer.decode(encoded, skip_special_tokens=True)\n",
        "    final_length = len(tokenizer.encode(final_prompt_string)) # Verify final length\n",
        "\n",
        "    # Handle potential small discrepancies due to decoding/re-encoding edge cases\n",
        "    if final_length != config.max_input_length:\n",
        "         print(f\"Warning: Final prompt length ({final_length}) differs slightly from target ({config.max_input_length}). Adjusting...\")\n",
        "         # Simple truncation as last resort\n",
        "         encoded = tokenizer.encode(final_prompt_string)[:config.max_input_length]\n",
        "         final_prompt_string = tokenizer.decode(encoded, skip_special_tokens=True)\n",
        "         final_length = len(tokenizer.encode(final_prompt_string))\n",
        "\n",
        "    print(f\"Using benchmark prompt with token length: {final_length}\")\n",
        "    prompts = [final_prompt_string] * config.num_benchmark_prompts\n",
        "    # It's often faster to give vLLM token IDs directly\n",
        "    prompt_token_ids = [encoded] * config.num_benchmark_prompts # Use the pre-calculated precise token list\n",
        "\n",
        "    # --- Configure vLLM Sampling Parameters ---\n",
        "    sampling_params = SamplingParams(\n",
        "        n=1, # Number of output sequences per prompt\n",
        "        temperature=0.0, # Deterministic for benchmark consistency\n",
        "        top_p=1.0,\n",
        "        max_tokens=config.max_output_length,\n",
        "        # ignore_eos=True, # Set True if you MUST generate exactly max_output_length tokens\n",
        "                           # Set False (default) allows stopping early on EOS token. Benchmark might vary slightly.\n",
        "        skip_special_tokens=True # Usually desired\n",
        "    )\n",
        "\n",
        "    # --- Prepare LoRA Request if enabled ---\n",
        "    lora_request = None\n",
        "    if config.enable_lora and config.lora_modules:\n",
        "        # Assuming only one LoRA for the benchmark run\n",
        "        lora_info = config.lora_modules[0]\n",
        "        lora_request = LoRARequest(\n",
        "            lora_name=lora_info[\"lora_name\"],\n",
        "            lora_int_id=lora_info[\"lora_int_id\"]\n",
        "        )\n",
        "        print(f\"Using LoRA request: {lora_request}\")\n",
        "\n",
        "\n",
        "    # --- Run Inference ---\n",
        "    print(f\"\\nSubmitting {len(prompts)} prompts to vLLM engine...\")\n",
        "    torch.cuda.synchronize() # Ensure previous work is done\n",
        "    start_time = time.time()\n",
        "\n",
        "    # vLLM's core generate call - handles batching and concurrency internally\n",
        "    vllm_outputs = llm.generate(\n",
        "        prompts=None, # Use prompt_token_ids for potential speedup\n",
        "        sampling_params=sampling_params,\n",
        "        prompt_token_ids=prompt_token_ids,\n",
        "        lora_request=lora_request # Pass LoRA request if enabled\n",
        "        )\n",
        "\n",
        "    torch.cuda.synchronize() # Ensure generation is complete\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    print(f\"vLLM generation finished in {total_time:.2f} seconds.\")\n",
        "\n",
        "    # --- Process Results and Calculate Metrics ---\n",
        "    results = []\n",
        "    total_input_tokens_processed = 0\n",
        "    total_output_tokens_processed = 0\n",
        "\n",
        "    for i, output in enumerate(vllm_outputs):\n",
        "        # vLLM output object contains prompt, generated text, token ids, etc.\n",
        "        req_output = RequestOutput(id=i, prompt=output.prompt) # Original prompt string\n",
        "        req_output.start_time = start_time # Use overall batch start time\n",
        "        req_output.end_time = end_time # Use overall batch end time\n",
        "        req_output.result = output.outputs[0].text # The generated text\n",
        "\n",
        "        input_len = len(output.prompt_token_ids)\n",
        "        output_len = len(output.outputs[0].token_ids)\n",
        "        total_input_tokens_processed += input_len\n",
        "        total_output_tokens_processed += output_len\n",
        "\n",
        "        # Store individual metrics if needed, though aggregate is primary\n",
        "        req_output.metrics = {\n",
        "            \"input_tokens\": input_len,\n",
        "            \"output_tokens\": output_len,\n",
        "            \"total_tokens\": input_len + output_len,\n",
        "        }\n",
        "        results.append(req_output)\n",
        "\n",
        "    # Calculate aggregate metrics using the dedicated function\n",
        "    metrics = calculate_vllm_aggregate_metrics(\n",
        "        results, start_time, end_time, total_input_tokens_processed, total_output_tokens_processed\n",
        "    )\n",
        "\n",
        "    # --- Print Benchmark Summary ---\n",
        "    print(\"\\n--- vLLM Benchmark Results ---\")\n",
        "    print(f\"Total requests submitted: {config.num_benchmark_prompts}\")\n",
        "    print(f\"Successful requests processed: {metrics.get('successful_requests', 0)}\")\n",
        "    print(f\"Total input tokens processed: {metrics.get('total_input_tokens', 0)}\")\n",
        "    print(f\"Total output tokens processed: {metrics.get('total_output_tokens', 0)}\")\n",
        "    print(f\"Total tokens (in+out): {metrics.get('total_tokens', 0)}\")\n",
        "    print(f\"Total wall time: {metrics.get('total_time_seconds', 0):.2f} seconds\")\n",
        "    print(f\"Aggregate throughput: {metrics.get('aggregate_throughput', 0):.2f} tokens/sec\")\n",
        "    print(\"---------------------------------\")\n",
        "    print(f\"Target throughput: 200 tokens/sec\")\n",
        "\n",
        "    if metrics.get('aggregate_throughput', 0) >= 200:\n",
        "        print(\"✅ BENCHMARK PASSED: Throughput meets or exceeds target!\")\n",
        "    else:\n",
        "        print(\"❌ BENCHMARK FAILED: Throughput below target.\")\n",
        "        print(\"   Consider adjusting vLLM parameters like 'gpu_memory_utilization',\")\n",
        "        print(\"   'max_num_batched_tokens', or trying explicit quantization ('awq', 'gptq').\")\n",
        "\n",
        "    return metrics, results"
      ],
      "metadata": {
        "id": "kb3Mt9wRvoMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Aggregate Metrics Calculation Function for vLLM\n",
        "def calculate_vllm_aggregate_metrics(\n",
        "    results: List[RequestOutput],\n",
        "    start_time: float,\n",
        "    end_time: float,\n",
        "    total_input_tokens: int,\n",
        "    total_output_tokens: int\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Calculate aggregate metrics for a vLLM benchmark run.\n",
        "    Uses overall start/end time and pre-summed tokens for accuracy.\n",
        "    \"\"\"\n",
        "    total_requests_processed = len(results)\n",
        "    total_time_seconds = end_time - start_time\n",
        "    total_tokens_processed = total_input_tokens + total_output_tokens\n",
        "\n",
        "    aggregate_throughput = total_tokens_processed / total_time_seconds if total_time_seconds > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"total_input_tokens\": total_input_tokens,\n",
        "        \"total_output_tokens\": total_output_tokens,\n",
        "        \"total_tokens\": total_tokens_processed,\n",
        "        \"total_time_seconds\": total_time_seconds,\n",
        "        \"aggregate_throughput\": aggregate_throughput,\n",
        "        \"successful_requests\": total_requests_processed,\n",
        "    }"
      ],
      "metadata": {
        "id": "gLpaGHJ-vqfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Main Execution Function (FINAL - Returns Objects, No Interactive Loop)\n",
        "def main():\n",
        "    # --- Configuration for Maximum Throughput ---\n",
        "    # MUST use a quantized model compatible with vLLM and T4 VRAM\n",
        "    # AWQ is recommended for speed. Ensure this exact model ID exists and is compatible.\n",
        "    default_model = \"TheBloke/Mistral-7B-Instruct-v0.1-AWQ\"\n",
        "    detected_quantization = \"awq\" # Set based on the chosen model ID\n",
        "\n",
        "    # Override vLLM defaults potentially beneficial for T4 and this task\n",
        "    # Limit model length to slightly more than needed (128+128=256) to save KV cache memory\n",
        "    max_len_override = 512\n",
        "    # Max batched tokens: Let vLLM decide initially, adjust if OOM occurs during generation\n",
        "    max_batched_tokens_override = None # Keep None for now\n",
        "\n",
        "    config = BenchmarkConfig(\n",
        "        model_id=default_model,\n",
        "        enable_lora=False, # Explicitly disable LoRA\n",
        "        quantization=detected_quantization, # Use the quantization type of the model\n",
        "        max_model_len_override=max_len_override,\n",
        "        max_num_batched_tokens_override=max_batched_tokens_override\n",
        "        )\n",
        "\n",
        "    print(\"--- Configuration for Max Throughput (Quantized) ---\")\n",
        "    print(f\"Model ID: {config.model_id}\")\n",
        "    print(f\"Quantization: {config.quantization}\")\n",
        "    print(f\"LoRA Disabled\")\n",
        "    if config.max_model_len_override:\n",
        "         print(f\"Max Model Length Override: {config.max_model_len_override}\")\n",
        "    if config.max_num_batched_tokens_override:\n",
        "         print(f\"Max Batched Tokens Override: {config.max_num_batched_tokens_override}\")\n",
        "    print(\"----------------------------------------------------\")\n",
        "\n",
        "    # --- Load Tokenizer ---\n",
        "    print(f\"\\nLoading tokenizer for {config.model_id}...\")\n",
        "    try:\n",
        "        # AWQ models often need legacy=False\n",
        "        tokenizer = AutoTokenizer.from_pretrained(config.model_id, trust_remote_code=True, legacy=False)\n",
        "    except TypeError:\n",
        "         print(\"Tokenizer does not support 'legacy' argument, loading without it.\")\n",
        "         tokenizer = AutoTokenizer.from_pretrained(config.model_id, trust_remote_code=True)\n",
        "\n",
        "\n",
        "    # Add padding token if missing\n",
        "    if tokenizer.pad_token is None:\n",
        "        if tokenizer.eos_token is not None:\n",
        "            print(\"Warning: No pad token found, using EOS token as pad token.\")\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "        else:\n",
        "             print(\"Warning: No pad or EOS token found. Adding a new pad token '[PAD]'.\")\n",
        "             tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    tokenizer.padding_side = 'left'\n",
        "    print(\"Tokenizer loaded.\")\n",
        "    print(f\"Padding side: {tokenizer.padding_side}\")\n",
        "    print(f\"Pad token: '{tokenizer.pad_token}', ID: {tokenizer.pad_token_id}\")\n",
        "\n",
        "    # --- Initialize vLLM Engine ---\n",
        "    print(f\"\\nInitializing vLLM engine with AWQ model...\")\n",
        "    # Prepare arguments\n",
        "    llm_args = {\n",
        "        \"model\": config.model_id,\n",
        "        \"tokenizer\": config.model_id,\n",
        "        \"tensor_parallel_size\": config.tensor_parallel_size,\n",
        "        \"gpu_memory_utilization\": config.gpu_memory_utilization,\n",
        "        \"quantization\": config.quantization, # Crucial: Pass 'awq'\n",
        "        \"trust_remote_code\": True,\n",
        "        \"enable_lora\": config.enable_lora,\n",
        "        \"max_loras\": config.max_loras,\n",
        "        \"max_lora_rank\": config.max_lora_rank,\n",
        "        \"dtype\": 'half' # Explicitly set to half/float16 for AWQ & T4\n",
        "    }\n",
        "    if config.max_model_len_override:\n",
        "        llm_args[\"max_model_len\"] = config.max_model_len_override\n",
        "    if config.max_num_batched_tokens_override:\n",
        "        llm_args[\"max_num_batched_tokens\"] = config.max_num_batched_tokens_override\n",
        "\n",
        "    llm_engine = None\n",
        "    try:\n",
        "        start_init = time.time()\n",
        "        llm_engine = LLM(**llm_args)\n",
        "        end_init = time.time()\n",
        "        print(f\"vLLM Engine Initialized in {end_init - start_init:.2f} seconds.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n--- ERROR initializing vLLM Engine ---\")\n",
        "        print(e)\n",
        "        print(\"Please check model ID, AWQ compatibility (vLLM needs specific dependencies for AWQ), and VRAM.\")\n",
        "        # Return None values if initialization fails\n",
        "        return None, None, None\n",
        "\n",
        "    # --- Run Benchmark ---\n",
        "    # Note: benchmark function uses the passed tokenizer, config, llm_engine\n",
        "    benchmark_metrics, benchmark_results = run_vllm_benchmark(tokenizer, config, llm_engine)\n",
        "\n",
        "    # --- Main function is done, return objects needed for interactive mode ---\n",
        "    print(\"\\n--- Benchmark Complete ---\")\n",
        "    return llm_engine, tokenizer, config # Return engine, tokenizer, config"
      ],
      "metadata": {
        "id": "FMqwiwvhvsBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Script Entry Point (FINAL - Includes Interactive Loop)\n",
        "if __name__ == \"__main__\":\n",
        "    llm_engine = None # Initialize to None\n",
        "    tokenizer = None\n",
        "    config = None\n",
        "\n",
        "    try:\n",
        "        # Run main logic (init, benchmark) and get objects\n",
        "        # This call might take a long time due to downloads and initialization\n",
        "        print(\"Starting main function for setup and benchmark...\")\n",
        "        llm_engine, tokenizer, config = main()\n",
        "        print(\"Main function execution finished.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n--- An critical error occurred during main execution ---\")\n",
        "        print(e)\n",
        "        # import traceback\n",
        "        # traceback.print_exc()\n",
        "\n",
        "    # --- Start Interactive Mode AFTER main() if successful ---\n",
        "    if llm_engine and tokenizer and config:\n",
        "        print(\"\\n\\n--- Interactive Mode (vLLM) ---\")\n",
        "        print(\"Benchmark complete. Interactive mode starting.\")\n",
        "        print(\"Type 'exit' or press Ctrl+C to quit.\")\n",
        "\n",
        "        # Prepare sampling parameters for interactive use\n",
        "        interactive_sampling_params = SamplingParams(\n",
        "            n=1,              # Generate one completion\n",
        "            temperature=0.7,  # Allow some creativity\n",
        "            top_p=0.9,        # Nucleus sampling\n",
        "            max_tokens=config.max_output_length # Use max output length from config\n",
        "        )\n",
        "\n",
        "        # Prepare LoRA request if LoRA was enabled during config setup\n",
        "        # (Note: It's currently disabled by default in main for the max throughput run)\n",
        "        interactive_lora_request = None\n",
        "        if config.enable_lora and config.lora_modules:\n",
        "             # Need to import LoRARequest if re-enabling LoRA\n",
        "             # from vllm import LoRARequest # Uncomment if LoRA is enabled\n",
        "             lora_info = config.lora_modules[0]\n",
        "             interactive_lora_request = LoRARequest(\n",
        "                lora_name=lora_info[\"lora_name\"],\n",
        "                lora_int_id=lora_info[\"lora_int_id\"]\n",
        "            )\n",
        "             print(f\"Interactive mode using LoRA: {interactive_lora_request.lora_name}\")\n",
        "\n",
        "\n",
        "        # The interactive loop\n",
        "        while True:\n",
        "            try:\n",
        "                user_input = input(\"\\nEnter prompt: \")\n",
        "                if user_input.lower() == 'exit':\n",
        "                    break\n",
        "                if not user_input.strip(): # Handle empty input\n",
        "                    continue\n",
        "\n",
        "                # Generate response using the vLLM engine\n",
        "                iter_start_time = time.time()\n",
        "                vllm_outputs = llm_engine.generate(\n",
        "                    prompts=[user_input], # Pass prompt as a list\n",
        "                    sampling_params=interactive_sampling_params,\n",
        "                    lora_request=interactive_lora_request # Pass None if LoRA disabled\n",
        "                    )\n",
        "                iter_end_time = time.time()\n",
        "\n",
        "                # Extract and display results\n",
        "                output = vllm_outputs[0] # Get the result for the first prompt\n",
        "                response = output.outputs[0].text\n",
        "\n",
        "                # Calculate performance metrics for this specific interaction\n",
        "                input_tokens = len(output.prompt_token_ids)\n",
        "                output_tokens = len(output.outputs[0].token_ids)\n",
        "                total_tokens = input_tokens + output_tokens\n",
        "                inference_time = iter_end_time - iter_start_time\n",
        "                throughput = total_tokens / inference_time if inference_time > 0 else 0\n",
        "\n",
        "                print(\"\\nResponse:\")\n",
        "                print(response)\n",
        "                print(\"\\n--- Performance ---\")\n",
        "                print(f\"Input tokens: {input_tokens}\")\n",
        "                print(f\"Output tokens: {output_tokens}\")\n",
        "                print(f\"Time: {inference_time:.2f} seconds\")\n",
        "                print(f\"Throughput: {throughput:.2f} tokens/sec\")\n",
        "\n",
        "            except KeyboardInterrupt: # Allow graceful exit with Ctrl+C\n",
        "                print(\"\\nExiting interactive mode due to KeyboardInterrupt.\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"\\nAn error occurred during interactive generation: {e}\")\n",
        "                # Consider adding more robust error handling if needed\n",
        "                time.sleep(1) # Prevent rapid error loops in case of persistent issues\n",
        "\n",
        "        print(\"\\n--- Interactive Mode Finished ---\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\nInteractive mode skipped due to error during main execution or missing objects.\")\n",
        "\n",
        "    # Final cleanup (optional, good practice)\n",
        "    print(\"\\n--- Script Execution Complete ---\")\n",
        "    print(\"Attempting final cleanup...\")\n",
        "    del llm_engine # Allow GC to collect the engine\n",
        "    del tokenizer\n",
        "    del config\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    print(\"Cleanup attempted.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f485a650dc8c44bca56cf437d48f86a5",
            "04be03772ee246ffbbd506ce265e9055",
            "b29cb7127b4c4938845ea6fe7e94934e",
            "d36b6683fcf64b0cb5514ca21f2b94d4",
            "b2c8b20a018a432a81191e3958a896df",
            "5cd8b6bf57cb40fc9c92b638c6a12aa0",
            "ad8c084d382e4d5db8f08660355f62fa",
            "d02f81d1937a474ab951c0d4a2633bf9",
            "63300c4defbc4b2095b221c2228eaa0f",
            "3800e330caab45ceb46e6879e30a7b03",
            "7642958c8f1e41d5bae746f34e1ce2c5"
          ]
        },
        "id": "vdybTKWYvvwn",
        "outputId": "3e2bfa5b-4b8b-43b8-aae7-71b6c6e8bdb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting main function for setup and benchmark...\n",
            "--- Configuration for Max Throughput (Quantized) ---\n",
            "Model ID: TheBloke/Mistral-7B-Instruct-v0.1-AWQ\n",
            "Quantization: awq\n",
            "LoRA Disabled\n",
            "Max Model Length Override: 512\n",
            "----------------------------------------------------\n",
            "\n",
            "Loading tokenizer for TheBloke/Mistral-7B-Instruct-v0.1-AWQ...\n",
            "Warning: No pad token found, using EOS token as pad token.\n",
            "Tokenizer loaded.\n",
            "Padding side: left\n",
            "Pad token: '</s>', ID: 2\n",
            "\n",
            "Initializing vLLM engine with AWQ model...\n",
            "WARNING 04-07 20:41:31 [config.py:2704] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 04-07 20:41:31 [config.py:600] This model supports multiple tasks: {'classify', 'score', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.\n",
            "WARNING 04-07 20:41:31 [config.py:679] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
            "INFO 04-07 20:41:31 [llm_engine.py:242] Initializing a V0 LLM engine (v0.8.3) with config: model='TheBloke/Mistral-7B-Instruct-v0.1-AWQ', speculative_config=None, tokenizer='TheBloke/Mistral-7B-Instruct-v0.1-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=512, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=TheBloke/Mistral-7B-Instruct-v0.1-AWQ, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
            "INFO 04-07 20:41:32 [model_runner.py:1110] Starting to load model TheBloke/Mistral-7B-Instruct-v0.1-AWQ...\n",
            "INFO 04-07 20:41:32 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
            "INFO 04-07 20:41:33 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f485a650dc8c44bca56cf437d48f86a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 04-07 20:41:44 [loader.py:447] Loading weights took 11.57 seconds\n",
            "INFO 04-07 20:41:44 [model_runner.py:1146] Model loading took 3.8735 GiB and 12.183108 seconds\n",
            "INFO 04-07 20:41:47 [worker.py:267] Memory profiling takes 1.72 seconds\n",
            "INFO 04-07 20:41:47 [worker.py:267] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.90) = 13.27GiB\n",
            "INFO 04-07 20:41:47 [worker.py:267] model weights take 3.87GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 0.38GiB; the rest of the memory reserved for KV Cache is 9.02GiB.\n",
            "INFO 04-07 20:41:47 [executor_base.py:112] # cuda blocks: 4617, # CPU blocks: 2048\n",
            "INFO 04-07 20:41:47 [executor_base.py:117] Maximum concurrency for 512 tokens per request: 144.28x\n",
            "INFO 04-07 20:41:48 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:47<00:00,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 04-07 20:42:36 [model_runner.py:1598] Graph capturing finished in 48 secs, took 0.31 GiB\n",
            "INFO 04-07 20:42:36 [llm_engine.py:448] init engine (profile, create kv cache, warmup model) took 51.15 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "<ipython-input-37-2ca95955be18>:92: DeprecationWarning: The keyword arguments {'prompt_token_ids'} are deprecated and will be removed in a future update. Please use the 'prompts' parameter instead.\n",
            "  benchmark_metrics, benchmark_results = run_vllm_benchmark(tokenizer, config, llm_engine)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vLLM Engine Initialized in 65.91 seconds.\n",
            "\n",
            "=== Running vLLM Benchmark ===\n",
            "Model: TheBloke/Mistral-7B-Instruct-v0.1-AWQ\n",
            "Target Concurrency: 32\n",
            "Input/Output Length: 128/128\n",
            "Using benchmark prompt with token length: 128\n",
            "\n",
            "Submitting 32 prompts to vLLM engine...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 32/32 [00:14<00:00,  2.23it/s, est. speed input: 285.05 toks/s, output: 285.05 toks/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vLLM generation finished in 14.38 seconds.\n",
            "\n",
            "--- vLLM Benchmark Results ---\n",
            "Total requests submitted: 32\n",
            "Successful requests processed: 32\n",
            "Total input tokens processed: 4096\n",
            "Total output tokens processed: 4096\n",
            "Total tokens (in+out): 8192\n",
            "Total wall time: 14.38 seconds\n",
            "Aggregate throughput: 569.57 tokens/sec\n",
            "---------------------------------\n",
            "Target throughput: 200 tokens/sec\n",
            "✅ BENCHMARK PASSED: Throughput meets or exceeds target!\n",
            "\n",
            "--- Benchmark Complete ---\n",
            "Main function execution finished.\n",
            "\n",
            "\n",
            "--- Interactive Mode (vLLM) ---\n",
            "Benchmark complete. Interactive mode starting.\n",
            "Type 'exit' or press Ctrl+C to quit.\n",
            "\n",
            "Enter prompt: what is life\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s, est. speed input: 4.69 toks/s, output: 25.81 toks/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Response:\n",
            " expectancy for a 38 year old male in usa?\n",
            "A: 78 years\n",
            "\n",
            "--- Performance ---\n",
            "Input tokens: 4\n",
            "Output tokens: 22\n",
            "Time: 0.86 seconds\n",
            "Throughput: 30.29 tokens/sec\n",
            "\n",
            "Enter prompt: what is life?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.50s/it, est. speed input: 2.00 toks/s, output: 28.02 toks/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Response:\n",
            "\n",
            "\n",
            "A: Life is the quality or state of being alive; the existence of an individual organism. It is characterized by biological processes such as growth, reproduction, and response to stimuli, and by the ability to adapt and evolve. The meaning and purpose of life are often debated and may vary from person to person.\n",
            "\n",
            "--- Performance ---\n",
            "Input tokens: 5\n",
            "Output tokens: 70\n",
            "Time: 2.50 seconds\n",
            "Throughput: 29.97 tokens/sec\n",
            "\n",
            "Enter prompt: potato chips\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.53s/it, est. speed input: 0.88 toks/s, output: 28.24 toks/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Response:\n",
            "\n",
            "\n",
            "# How to make your own potato chips\n",
            "\n",
            "I've been making my own potato chips for a long time. It's a simple process that I love because I can control the salt, the thickness, and the flavor. I've been making them in the oven for years, but I recently tried making them in an air fryer and I was amazed by the results.\n",
            "\n",
            "Here's how to make your own potato chips in an air fryer:\n",
            "\n",
            "Ingredients:\n",
            "- 2 large russet potatoes\n",
            "- 2 tablespoons of veget\n",
            "\n",
            "--- Performance ---\n",
            "Input tokens: 4\n",
            "Output tokens: 128\n",
            "Time: 4.54 seconds\n",
            "Throughput: 29.09 tokens/sec\n",
            "\n",
            "Enter prompt: isguefbjiebjivjiwefb\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.47s/it, est. speed input: 2.46 toks/s, output: 28.65 toks/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Response:\n",
            "jiejeiejejivjiejejiejejiejejiejejejiejejiejejiejejiejejiejejiejejiejejiejejiejejiejejiejejiejejiejejiejejiejejiejejiejejiejejiejejiejejiejejiejejiejejiejejiejejiejejiejejiejejiejejiejejiejejiejejiejejiejejiejejieje\n",
            "\n",
            "--- Performance ---\n",
            "Input tokens: 11\n",
            "Output tokens: 128\n",
            "Time: 4.47 seconds\n",
            "Throughput: 31.08 tokens/sec\n",
            "\n",
            "Enter prompt: Gaana logo  Search Artists, Songs, Albums Welcome Offer: 1 Month Trial @ ₹1 Get Gaana Plus  A अ  Log In / Sign Up All Trending Songs New Songs Old Songs Moods & Genres Party Romance 90s & 2000s Bhakti Indie EDM Ghazals Workout Stars Retro Album Top Playlist Top Hindi Songs Top Haryanvi Songs Top Punjabi Songs Top Tamil Songs Top Bhakti Songs 90's Hindi Songs Top English Songs Top Bhojpuri Songs Top Artist Arijit Singh Sonu Nigam Sidhu Moose Wala A. R. Rahman Yo Yo Honey Singh Badshah MC Stan Lata Mangeshkar Diljit Dosanjh Shreya Ghoshal Radio Podcast Gaana Charts My Music Reads Gaana Hindi Songs Jab Koi Baat Song Jab Koi Baat Lyrics blurImg Jab Koi Baat Lyrics Jab Koi Baat Lyrics Bollywood Romantic Hits - With Shayaris2017  Kumar Sanu, Sadhana Sargam, Minalini Singh 7 min 57 sec Play Song   Jab Koi Baat Lyrics Jab Koi Baat Lyrics Play Song   Lyrics Jab koi baat bigad jaaye jab koi mushkil pad jaaye  Tum dena saath mera o humnawaaz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.93s/it, est. speed input: 67.00 toks/s, output: 25.99 toks/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Response:\n",
            "  Mera o humnawaaz mein  Aapko isliye  Tumko isliye  Tumko isliye  Tumko isliye  Tumko isliye  Tumko isliye  Tumko isliye  Tumko isliye  Tumko isliye  Tumko isliye  Tumko isliye  Tumko isliye  Tumko isliye  Tumko isliye  Tumko isliye  Tumko isliye  Tumko isli\n",
            "\n",
            "--- Performance ---\n",
            "Input tokens: 330\n",
            "Output tokens: 128\n",
            "Time: 4.93 seconds\n",
            "Throughput: 92.89 tokens/sec\n",
            "\n",
            "Enter prompt: what if you are\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.01s/it, est. speed input: 2.49 toks/s, output: 28.84 toks/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Response:\n",
            " an 18 year old that has been diagnosed with cancer and has to undergo a major surgery, how would you feel?\n",
            "\n",
            "if you are an 18 year old that has been diagnosed with cancer and has to undergo a major surgery, how would you feel?\n",
            "\n",
            "--- Performance ---\n",
            "Input tokens: 5\n",
            "Output tokens: 58\n",
            "Time: 2.02 seconds\n",
            "Throughput: 31.22 tokens/sec\n",
            "\n",
            "Enter prompt: i wonder why kanye\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.44s/it, est. speed input: 1.58 toks/s, output: 28.86 toks/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Response:\n",
            " west is getting a lot of hate from the media and the public, it seems like everytime he opens his mouth people just want to shut him down and call him a racist. i think kanye is just trying to be a normal person and do what he loves, but people just want to judge him and label him as a racist.\n",
            "\n",
            "i think people just want to shut kanye down because he is a celebrity and people think that celebrities are not allowed to have opinions or speak their minds. but i think that is ridiculous, everyone should be able to express themselves and have their own opinions.\n",
            "\n",
            "\n",
            "\n",
            "--- Performance ---\n",
            "Input tokens: 7\n",
            "Output tokens: 128\n",
            "Time: 4.44 seconds\n",
            "Throughput: 30.40 tokens/sec\n",
            "\n",
            "Enter prompt: kanye is dumb as fuck\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.44s/it, est. speed input: 1.80 toks/s, output: 28.83 toks/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Response:\n",
            ". he has a mental disorder. he's not a musician. he's a puppet. he's a poser. he's a troll. he's a poser. he's a poser. he's a poser. he's a poser. he's a poser. he's a poser. he's a poser. he's a poser. he's a poser. he's a poser. he's a poser. he's a poser. he's a poser. he'\n",
            "\n",
            "--- Performance ---\n",
            "Input tokens: 8\n",
            "Output tokens: 128\n",
            "Time: 4.45 seconds\n",
            "Throughput: 30.59 tokens/sec\n",
            "\n",
            "Enter prompt: nsfw roleplau\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.42s/it, est. speed input: 1.58 toks/s, output: 28.98 toks/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Response:\n",
            "\n",
            "\n",
            "This is a nsfw roleplay.\n",
            "\n",
            "Please be aware that the content within this roleplay may be mature or offensive.\n",
            "\n",
            "If you are not comfortable with such content, please proceed with caution.\n",
            "\n",
            "If you are under the age of 18, please do not enter this roleplay.\n",
            "\n",
            "If you enter this roleplay, you agree to the following terms:\n",
            "\n",
            "* I am of legal age to enter this roleplay.\n",
            "* I am comfortable with mature or offensive content.\n",
            "* I understand that this roleplay is nsfw and may not be suitable for all audiences.\n",
            "\n",
            "--- Performance ---\n",
            "Input tokens: 7\n",
            "Output tokens: 128\n",
            "Time: 4.42 seconds\n",
            "Throughput: 30.54 tokens/sec\n",
            "\n",
            "Enter prompt: exit\n",
            "\n",
            "--- Interactive Mode Finished ---\n",
            "\n",
            "--- Script Execution Complete ---\n",
            "Attempting final cleanup...\n",
            "Cleanup attempted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BvAZemfQv2lP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}